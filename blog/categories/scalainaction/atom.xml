<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ScalaInAction | 崔鹏飞的Octopress Blog]]></title>
  <link href="http://cuipengfei.github.com/blog/categories/scalainaction/atom.xml" rel="self"/>
  <link href="http://cuipengfei.github.com/"/>
  <updated>2015-06-02T21:05:24+05:30</updated>
  <id>http://cuipengfei.github.com/</id>
  <author>
    <name><![CDATA[崔鹏飞]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark RDD的fold和aggregate为什么是两个API？为什么不是一个foldLeft？]]></title>
    <link href="http://cuipengfei.github.com/blog/2014/10/31/spark-fold-aggregate-why-not-foldleft/"/>
    <updated>2014-10-31T15:29:00+05:30</updated>
    <id>http://cuipengfei.github.com/blog/2014/10/31/spark-fold-aggregate-why-not-foldleft</id>
    <content type="html"><![CDATA[<p>大家都知道Scala标准库的List有一个用来做聚合操作的foldLeft方法。</p>

<p>比如我定义一个公司类：</p>

<p><code>scala
case class Company(name:String, children:Seq[Company]=Nil)
</code>
它有名字和子公司。
然后定义几个公司：</p>

<p><code>scala
val companies = List(Company("B"),Company("A"),Company("T"))
</code></p>

<p>三家大公司，然后呢，我假设有一家超牛逼的公司把它们给合并了：</p>

<p><code>scala
companies.foldLeft(Company("King"))((king,company)=&gt;Company(name=king.name,king.children:+company))
</code></p>

<p>这个执行的结果是这样的：</p>

<p><code>scala
scala&gt; companies.foldLeft(Company("King"))((king,company)=&gt;Company(name=king.name,king.children:+company))
res6: Company = Company(King,List(Company(B,List()), Company(A,List()), Company(T,List())))
</code></p>

<p>可见foldLeft的结果是一家包含了BAT三大家得新公司。</p>

<p>由List[Company]聚合出一个新的Company，这种属于foldLeft的同构聚合操作。</p>

<p>同时，foldLeft也可以做异构的聚合操作：</p>

<p><code>scala
companies.foldLeft("")((acc,company)=&gt;acc+company.name)
</code></p>

<p>它的执行结果是这样的：</p>

<p><code>scala
scala&gt; companies.foldLeft("")((acc,company)=&gt;acc+company.name)
res7: String = BAT
</code></p>

<p>由List[Company]聚合出一个String。</p>

<p>这样的API感觉很方便，只要是聚合，无论同构异构，都可以用它来做。</p>

<p>最近接触了Spark，其中的RDD是做分布式计算时最常用的一个类。</p>

<p>RDD有一个叫做fold的API，它和foldLeft的签名很像，唯一区别是它只能做同构聚合操作。</p>

<p>也就是说如果你有一个RDD[X]，通过fold，你只能构造出一个X。</p>

<p>如果我想通过一个RDD[X]构造一个Y出来呢？</p>

<p>那就得用aggregate这个API了，aggregate的签名是这样的：</p>

<p><code>scala
aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U
</code></p>

<p>它比fold和foldLeft多需要一个combOp做参数。</p>

<p>这让我很不解，同构和异构的API干嘛非得拆成两个呢？怎么不能学Scala的标准库，把它做成类似foldLeft的样子呢？</p>

<p>后来想明白了，这是由于Spark需要分布运算造成的。</p>

<p>先想一下Scala List的foldLeft是怎么工作的？</p>

<p><code>scala
companies.foldLeft(Company("King"))((king,company)=&gt;Company(name=king.name,king.children:+company))
</code></p>

<ol>
<li>拿到初始值，即名字为king的公司，把它和list中的第一个公司合并，成为一个包含一家子公司的新公司</li>
<li>把上一步中的新公司拿来和list中的第二个公司合并，成为一个包含两家子公司的新公司</li>
<li>把上一步中的新公司拿来和list中的第三个公司合并，成为一个包含三家子公司的新公司</li>
</ol>


<p>这是同构的过程。</p>

<p><code>scala
companies.foldLeft("")((acc,company)=&gt;acc+company.name)
</code></p>

<ol>
<li>拿到初始值，即空字符串，把它和list中的第一个公司的名字拼在一起，成为B</li>
<li>把上一步中的B第二个公司名字拼一起，成为BA</li>
<li>把上一步中的BA拿来和list中的第三个公司的名字拼一起，成为BAT</li>
</ol>


<p>这是异构的过程。</p>

<p>像多米诺骨牌一样，从左到右依次把list中的元素吸收入结果中。</p>

<p>现在假设RDD[X]中有一个类似foldLeft的API，其签名和foldLeft一致，我现在调用foldLeft，给它一个f:(Y,X)=>Y，接下来该发生什么呢？</p>

<ol>
<li>因为要分布计算，所以我先要把手里的很多个X分成几份，分发到不同的节点上去</li>
<li>每个节点把拿到的很多个X计算出一个Y出来</li>
<li>把所有节点的结果拿来，这时我手里就有了很多个Y</li>
<li>啊。。。我不知道怎么把很多个Y变成一个Y啊。。。</li>
</ol>


<p>由于Spark的RDD不像Scala的List一样只需要推倒一副多米诺骨牌，而是要推倒很多副，最后再对很多副多米诺骨牌的结果做聚合。</p>

<p>这时如果是同构还好，我只需要再用f:(X,X)=>X做一遍就ok了。</p>

<p>但是如果是异构的，那我就必须得再需要一个f:(Y,Y)=>Y了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala中Stream的应用场景及其实现原理]]></title>
    <link href="http://cuipengfei.github.com/blog/2014/10/23/scala-stream-application-scenario-and-how-its-implemented/"/>
    <updated>2014-10-23T17:21:00+05:30</updated>
    <id>http://cuipengfei.github.com/blog/2014/10/23/scala-stream-application-scenario-and-how-its-implemented</id>
    <content type="html"><![CDATA[<h1>假设一个场景</h1>

<p>需要在50个随机数中找到前两个可以被3整除的数字。</p>

<p>听起来很简单，我们可以这样来写：</p>

<p>```scala
def randomList = (1 to 50).map(_ => Random.nextInt(100)).toList</p>

<p>def isDivisibleBy3(n: Int) = {
  val isDivisible = n % 3 == 0
  println(s"$n $isDivisible")
  isDivisible
}</p>

<p>randomList.filter(isDivisibleBy3).take(2)
```</p>

<p>一个产生50个随机数的函数；</p>

<p>一个检查某数字是否能被3整除的函数；</p>

<p>最后，对含有50个随机数的List做filter操作，找到其中所有能够被3整除的数字，取其中前两个。</p>

<p>把这段代码在Scala的console里面跑一下，结果是这样的：</p>

<p><code>scala
scala&gt; randomList.filter(isDivisibleBy3).take(2)
31 false
71 false
95 false
7 false
38 false
48 true
88 false
52 false
2 false
27 true
90 true
55 false
96 true
91 false
82 false
83 false
8 false
51 true
96 true
27 true
12 true
76 false
17 false
53 false
54 true
70 false
29 false
49 false
12 true
83 false
18 true
6 true
7 false
76 false
51 true
95 false
76 false
85 false
87 true
84 true
44 false
44 false
89 false
84 true
42 true
44 false
0 true
23 false
35 false
55 false
res34: List[Int] = List(48, 27)
</code></p>

<p>其最终结果固然是没有问题，找到了48和27这两个数字。但是非常明显的可以看出，isDivisibleBy3被调用了50次，找到了远多于两个的能被3整除的数字，但是最后我们只关心其中前两个结果。</p>

<p>这似乎有点浪费，做了很多多余的运算。</p>

<p>对于这个例子来说，这还没什么，我们的List很小，判断整除于否也不是什么耗时操作。</p>

<p>但是如果List很大，filter时所做的运算很复杂的话，那这种做法就不可取了。</p>

<h1>现有解法的优缺点</h1>

<p><code>scala
randomList.filter(isDivisibleBy3).take(2)
</code>
这行代码有一个优点：</p>

<p>用描述性、声明性的语言描述了我们要做的事是什么，而无需描述怎么做。我们只需说先用filter过滤一下，然后拿前两个，整件事就完成了。</p>

<p>但是它同时也有一个缺点：</p>

<p>做了多余的运算，浪费资源，而且这个缺点会随着数据量的增大以及计算复杂度的增加而更为凸显。</p>

<h1>试着解决其缺点</h1>

<p>解决多余运算的思路很简单，不要过滤完整个List之后再取前两个。而是在过滤的过程中如果发现已经找到两个了，那剩下的就忽略掉不管了。</p>

<p>顺着这个思路很容易写出如下很像Java的代码：</p>

<p>```scala
  def first2UsingMutable: List[Int] = {</p>

<pre><code>val result = ListBuffer[Int]()

randomList.foreach(n =&gt; {
  if (isDivisibleBy3(n)) result.append(n)
  if (result.size == 2) return result.toList
})

result.toList
</code></pre>

<p>  }
```</p>

<p>创建一个可变的List，开始遍历随机数，找到能被3整除的就把它塞进可变List里面去，找够了两个就返回。</p>

<p>执行的结果如下：</p>

<p><code>scala
scala&gt; first2UsingMutable
31 false
89 false
21 true
29 false
12 true
res35: List[Int] = List(21, 12)
</code></p>

<p>可以看到，运算量确实变少了，找够了两个就直接收工了。</p>

<p>但是这实在很糟糕，显式使用了return同时还引入了可变量。</p>

<p>有什么东西像是一个foreach循环而又可以不引入可变量呢？fold</p>

<p>```scala
  def first2UsingFold: List[Int] = {</p>

<pre><code>randomList.foldLeft(Nil: List[Int])((acc, n) =&gt; {
  if (acc.size == 2) return acc
  if (isDivisibleBy3(n)) n :: acc
  else acc
})
</code></pre>

<p>  }
```</p>

<p>执行：</p>

<p><code>scala
scala&gt; first2UsingFold
98 false
77 false
68 false
93 true
93 true
res36: List[Int] = List(93, 93)
</code></p>

<p>效果和上面一段代码类似，没有多余的运算。但是由于需要early termination，所以还是摆脱不了return。</p>

<p>这两种解法在去除多余运算这个缺点的同时也把原来的优点给丢掉了，我们又退化回了描述如何做而不是做什么的程度了。</p>

<h1>如何保持代码的表意性而又不用做多余运算呢？</h1>

<p>其实类似的问题是有套路化的解决方案的：使用Stream。</p>

<p><code>scala
randomList.toStream.filter(isDivisibleBy3).take(2).toList
</code></p>

<p>这行代码执行的结果：</p>

<p><code>scala
scala&gt; randomList.toStream.filter(isDivisibleBy3).take(2).toList
86 false
15 true
53 false
20 false
93 true
res42: List[Int] = List(15, 93)
</code></p>

<p>可见没有多余运算了，而且这行代码和最初代码极为相似，都是通过描述先做filter再做take来完成任务的。缺点没有了，优点也保留了下来。</p>

<p>这同样都是filter和take，代码跟代码的差距咋就这么大呢？</p>

<p>答案就是：因为Stream利用了惰性求值（lazy evaluation），或者也可以称之为延迟执行（deferred execution）。</p>

<p>接下来就看一下这两个晦涩的名词是如何帮助Stream完成工作的吧。</p>

<h1>实现原理</h1>

<p>在这里我借用一下Functional programming in Scala这本书里对Stream实现的代码，之所以不用Scala标准库的源码是因为我们只需要实现filter，take和toList这三个方法就可以展示Stream的原理，就不需要动用重型武器了。</p>

<p>先假设我们自己实现了一个MyStream，它的用法和Stream是类似的：</p>

<p><code>scala
MyStream(randomList: _*).filter(isDivisibleBy3).take(2).toList
</code></p>

<p>以这一行代码为引子，我们来开始解剖MyStream是如何工作的。</p>

<h1>类型签名</h1>

<p>```scala
trait MyStream[+A] {</p>

<pre><code>. . . . . .
</code></pre>

<p>}</p>

<p>case object Empty extends MyStream[Nothing]</p>

<p>case class Cons<a href="h:%20(">+A</a> => A, t: () => MyStream[A]) extends MyStream[A]
```</p>

<p>一个trait叫做MyStream，其中的内容我们暂时忽略掉。</p>

<p>它有两个子类，一个Cons，一个Empty。Empty当然是代表空Stream了。</p>

<p>而Cons则是头尾结构的，头是Stream中的一个元素，尾是Stream中余下的元素。请注意头和尾这两个参数的类型并不是A，头的类型是一个能够返回A的函数，尾的类型是一个能够返回MyStream[A]的函数。</p>

<h1>初始化</h1>

<p>有了以上的类型定义以及头尾结构，我们就可以把很多个Cons加一个Empty（或者是无限多个Cons，没有Empty）连起来就构成一个Stream了，比如这样：</p>

<p><code>scala
Cons(()=&gt;1,()=&gt;Cons(()=&gt;2,()=&gt;Empty))
</code></p>

<p>这样就可以构造一个含有1，2的Stream了。</p>

<p>不过，请注意，上面的说法并不严谨，实际上它是一个包含着两个分别会返回1和2的函数的Stream。</p>

<p>也就是说当上面的代码在构造Cons的时候，1和2还没有“出生”，它们被包在一个函数里，等着被释放出来。</p>

<p>如果说我们通常熟知的一些集合包含的是花朵的话，那Stream所包含的就是花苞，它本身不是花，但是有开出花来的能力。</p>

<h1>Smart初始化</h1>

<p>当然，如果直接暴露Cons的构造函数出去给别人用的话，那这API也未免太不友好了，所以Stream需要提供一个易用的初始化的方式：</p>

<p>```scala
object MyStream {</p>

<p>  def apply<a href="elems:%20A*">A</a>: MyStream[A] = {</p>

<pre><code>if (elems.isEmpty) empty
else cons(elems.head, apply(elems.tail: _*))
</code></pre>

<p>  }</p>

<p>  def cons<a href="hd:%20=>%20A,%20tl:%20=>%20MyStream[A]">A</a>: MyStream[A] = {</p>

<pre><code>lazy val head = hd
lazy val tail = tl
Cons(() =&gt; head, () =&gt; tail)
</code></pre>

<p>  }</p>

<p>  def empty[A]: MyStream[A] = Empty
}
```</p>

<p>这个没有太多好解释的，我们就是用apply和小写的cons这两个方法来把客户代码原本要写的一大堆匿名函数给代劳掉。</p>

<p>需要注意的一点是apply方法看似是递归的，好像是你调用它的时候如果给它n个元素的话，它会自己调用自己n-1次。事实上它确实会调用自己n-1次，但是并不是立即发生的，为什么呢？</p>

<p>因为小写的cons方法所接受的第二个参数不是eager evaluation的，这就会使得apply(elems.tail: _*)这个表达式不会立即被求值。这就意味着，apply缺失会被调用n次，但是这n次并不是一次接一次连续发生的，它只会在我们对一个Cons的尾巴求值时才会发生一次。</p>

<p>如果说普通的集合中包含的是数据的话，那Stream中所包含的就是能够产生数据的算法。</p>

<p>如何？是不是花朵花苞的感觉又回来了？</p>

<p>还记得我们开始剖析的时候那句代码是什么吗？</p>

<p><code>scala
MyStream(randomList: _*).filter(isDivisibleBy3).take(2).toList
</code></p>

<p>现在我们算是把MyStream(randomList: _*)这一小点说清了。</p>

<p>接下来看MyStream(randomList: _*).filter(isDivisibleBy3)是如何work的。</p>

<h1>filter</h1>

<p>```scala
trait MyStream[+A] {</p>

<p>  def filter(p: A => Boolean): MyStream[A] = {</p>

<pre><code>this match {
  case Cons(h, t) =&gt;
    if (p(h())) cons(h(), t().filter(p))
    else t().filter(p)
  case Empty =&gt; empty
}
</code></pre>

<p>  }</p>

<p>. . . . . .</p>

<p>}</p>

<p>```</p>

<p>这个方法定义在基类里，又是一个看似递归的实现。</p>

<p>为什么说是看似呢？因为在</p>

<p><code>scala
if (p(h())) cons(h(), t().filter(p))
</code></p>

<p>这行代码中我们又用到了小写的cons，它所接受的参数不会被立即求值。也就是说filter一旦找到一个合适的元素，它就不再继续跑了，剩下的计算被延迟了。</p>

<p>比较值得提一下的是：这里的h()是什么呢？h是构造Cons时的第一个参数，它是什么类型的？()=>A。它就是之前提到的能够生产数据的算法，就是那个能够开出花朵的花苞。在这里我们说h()，就是在调用这个函数来拿到它所生产的数据，就是让一个花苞开出花朵。</p>

<h1>take</h1>

<p><code>scala
MyStream(randomList: _*).filter(isDivisibleBy3).take(2)
</code></p>

<p>接下来就该说take是如何work的了。在这里我们可以回顾一下，MyStream(randomList: _*)返回一个类型为MyStream[Int]，其中包含很多个可以返回Int的函数的容器。然后我们调用了这个容器的filter方法，filter又返回一个包含很多个可以返回Int的函数的容器。请注意，到这里为止，真正的计算还没有开始，真正的计算被包含到了一个又一个的函数（花苞）中，等待着被调用（绽放）。</p>

<p>那对filter的结果调用take又会怎样呢？</p>

<p>```scala
trait MyStream[+A] {</p>

<p>  . . . . . .</p>

<p>  def take(n: Int): MyStream[A] = {</p>

<pre><code>if (n &gt; 0) this match {
  case Cons(h, t) if n == 1 =&gt; cons(h(), MyStream.empty)
  case Cons(h, t) =&gt; cons(h(), t().take(n - 1))
  case _ =&gt; MyStream.empty
}
else MyStream()
</code></pre>

<p>  }</p>

<p>  . . . . . .</p>

<p>}</p>

<p>```</p>

<p>看过了前面的apply和filter之后，take就显得顺眼了很多。我们又见到了小写的cons，条件反射一般，我们就可以意识到，只要看见cons，那就意味着作为它的参数的表达式不会被立即求值，那这就意味着计算被放到了函数里，稍后再执行。那稍后到底是什么时候呢？</p>

<p>那就得看下面的toList了。</p>

<h1>toList</h1>

<p>```scala
trait MyStream[+A] {</p>

<p>  . . . . . .</p>

<p>  def toList: List[A] = {</p>

<pre><code>this match {
  case Cons(h, t) =&gt; h() :: t().toList
  case Empty =&gt; Nil
}
</code></pre>

<p>  }</p>

<p>}</p>

<p>```</p>

<p>又是一个递归实现，但是这次可不是看似递归了，这次是实打实的递归：只要还没有遇到空节点，就继续向后遍历。这次没有使用cons，没有任何计算被延迟执行，我们通过不断地对h()求值，来把整个Stream中每一个能够生产数据的函数都调用一遍以此来拿到我们最终想要的数据。</p>

<h1>总结</h1>

<p>要把以上的代码细节全部load进脑子跑一遍确实不太容易，我们人类的大脑栈空间太浅了。</p>

<p>所以我们试着从上面所罗列出的纷繁的事实中抽象出一些适合人脑理解的描述性语句吧：</p>

<ul>
<li>List(1,2,3)会构造一个容器，容器中包含数据</li>
<li>List(1,2,3).filter(n=>n>1)会构造出一个新的容器，其中包含2和3，这两块具体的数据</li>
<li><p>List(1,2,3).filter(n=>n>1).take(1)会把上一步中构造成的容器中的第一块数据取出，放入一个新容器</p></li>
<li><p>MyStream(1,2,3)也会构造一个容器，但是这个容器中不包含数据，它包含能够生产数据的算法</p></li>
<li>MyStream(1,2,3).filter(n=>n>1)也会构造出一个新的容器，这个容器中所包含的仍然是算法，是基于上一步构造出的能生产1，2，3的算法之上的判断数字是否大于1的算法</li>
<li>MyStream(1,2,3).filter(n=>n>1).take(1)会把上一步中构造成的算法容器中的第一个算法取出，放入一个新容器</li>
<li>MyStream(1,2,3).filter(n=>n>1).take(1).toList终于把上面所有步骤构造出的算法执行了，从而得到了最终想要的结果</li>
</ul>


<p>上面对List和Stream的应用的区别在哪儿呢？</p>

<p>就在于List是先把数据构造出来，然后在一堆数据中挑选我们心仪的数据。</p>

<p>而Stream是先把算法构造出来，挑选心仪的算法，最后只执行一大堆算法中我们需要的那一部分。</p>

<p>这样，自然就不会执行多余的运算了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[solving regular problems in scala]]></title>
    <link href="http://cuipengfei.github.com/blog/2014/09/06/solving-regular-problems-in-scala/"/>
    <updated>2014-09-06T20:28:00+05:30</updated>
    <id>http://cuipengfei.github.com/blog/2014/09/06/solving-regular-problems-in-scala</id>
    <content type="html"><![CDATA[<iframe src="http://cuipengfei.github.com//slides.com/pengfeicui/solving-regular-problems-in-scala/embed" width="800" height="600" scrolling="no" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在使用play framework的evolutions？需要支持SQL Server？用Liquibase吧]]></title>
    <link href="http://cuipengfei.github.com/blog/2014/07/18/play-evolutions-to-liquibase/"/>
    <updated>2014-07-18T15:11:00+05:30</updated>
    <id>http://cuipengfei.github.com/blog/2014/07/18/play-evolutions-to-liquibase</id>
    <content type="html"><![CDATA[<p>我所在的项目在用Scala + Play framework做一个web app。</p>

<p>Play自带的evolutions是一个DB Migration工具，从一开始我们就在用它来做所有阶段的数据迁移工作。</p>

<p>运行自动化测试时它可以帮每个测试用例在H2中创建数据（H2是Play默认的内存数据库）。
在下一个测试用例运行时evolutions则会创建一份和上次完全相同的新数据，这样我们的测试可以获得独立性而不用担心之前的测试遗留的副作用。也不用担心会给下一个测试遗留下什么脏数据。</p>

<p>在测试或者部署环境中运行时它也可以针对Postgres做数据迁移。</p>

<p>这一切看起来都挺好，我们就差喊evolutions是我们忠实的好伙伴了。</p>

<p>但是，快到给终端客户部署时，某一家客户提出他们一定要使用SQL Server，我们最初提出的使用Postgres他们不接受了。这时我们才发现evolutions的设计初衷就是在开发和测试阶段提供便利性，它根本就没想成为一个production ready的东西。</p>

<p>这样看来我们必须得寻找一个正经的DB Migration的工具了。而且这个DB Migration工具一定要满足以下几点：</p>

<ol>
<li>能够在运行自动化测试时和H2结合使用（因为我们已经有很多测试在依赖于H2跑了，要换掉成本较高）</li>
<li>能支持多种数据库（今天有人要SQL Server的支持，明天说不定还会有人要其他的）</li>
<li>在支持多种数据库时不需要我们写不同风格的SQL脚本（要写出让各个DB都不挑剔的SQL实在是太费劲了）</li>
</ol>


<p>我最先想到的就是Flyway，之前用过，而且TW的tech radar也提到过它。</p>

<p>但是它并没有入选，原因在于上面的第三点。Flyway要求使用者自己提供执行所需的SQL脚本。
这就意味着我们写SQL时需要同时兼顾H2，Postgres，SQL Server的异同。而且还无法预知未来的其他数据库会对我们现在写出的SQL脚本产生什么样的影响。</p>

<p>最后我们选择了Liquibase，我们可以通过JSON，YAML，或者XML来定义数据。Liquibase自己负责把我们定义的数据翻译给各种不同的数据库。</p>

<p>这样，通过一层中间语言。我们就隔离了数据库的差异对我们开发工作可能会造成的影响。</p>

<p>Ok，要用Liquibase这个大方向就确定了。但是具体怎么把它跑起来呢？在什么时机跑它呢？</p>

<p>用脚本跑？</p>

<p>Liquibase确实提供了Standalone，我们可以用脚本来调用它。</p>

<p>但是这怎么和build结合起来呀？在测试时调用它？在app启动时调用它？</p>

<p>那H2运行的端口每次都未必是一样的，这怎么办啊？</p>

<p>这个方案想想就费劲。</p>

<p>把它做成sbt的一个task？</p>

<p>这样确实比直接用脚本要稍微距离我们的build近一点，但是还是会有类似的问题。我们需要显式地去调用它，还要选择合适的时机去调用它。实现起来也会很麻烦。</p>

<p>而实际上，Play自己是支持plug in的。我们想要控制执行时机，而有谁比Play自己更了解它的运行时机呢？</p>

<p>而且已经有人做了liquibase play plug in。我把它fork了一份，更新了liquibase和play的版本，提高了log的level。并且部署到了sonatype去。</p>

<p>由于是Play自己的plug in，不是我们试图插入的生硬的脚本或者sbt task。Play自己知道该在什么合适的时机去执行它。</p>

<p>下面说一下如何应用它吧。</p>

<ul>
<li>在所有的conf文件中删掉所有和evolutions有关的配置</li>
</ul>


<p>这两个东西不能一起用，要不然我们需要同时维护两种DB Migration的脚本。</p>

<ul>
<li>在dependencies中加入这一项：</li>
</ul>


<p>"com.github.cuipengfei" % "play-liquibase_2.11" % "1.1"</p>

<p>很明显，这是用来引入这个plugin的。</p>

<ul>
<li>在conf目录下创建一个名为play.plugins的文件，在其中写入：</li>
</ul>


<p>400:com.github.cuipengfei.LiquibasePlugin</p>

<p>冒号前的400用来定义plugin的执行优先级，Play会由此决定何时执行该plugin。</p>

<p>冒号后是plugin的完全限定名。</p>

<ul>
<li>在你需要的conf文件中加入两行：</li>
</ul>


<p>liquibaseplugin=enabled</p>

<p>applyLiquibase.default=true</p>

<p>这样用来启用该plugin。</p>

<ul>
<li>在conf/liquibase/default/下创建一个modules.xml。</li>
</ul>


<p>在其中写入你的数据定义。（具体如何写，liquibase的官网有详细的介绍）</p>

<p>如果你用的数据库名字不是default，相应的替换就ok了。</p>

<p>这样，就大功告成了。</p>

<p>当你用sbt运行自动化测试时，liquibase会帮你创建数据。</p>

<p>当你在本地调试运行时，liquibase会帮你set up数据库。</p>

<p>当应用被部署到生产环境下去的时候，liquibase也可以帮你在第一次运行时进行数据的初创。</p>
]]></content>
  </entry>
  
</feed>
